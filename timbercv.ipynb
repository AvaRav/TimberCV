{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["8gw8XDMGWbqZ"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8457030,"sourceType":"datasetVersion","datasetId":4455142},{"sourceId":21102,"sourceType":"modelInstanceVersion","modelInstanceId":17478},{"sourceId":53223,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":44554}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Разметка","metadata":{"id":"8gw8XDMGWbqZ","execution":{"iopub.status.busy":"2024-02-18T10:42:46.496811Z","iopub.execute_input":"2024-02-18T10:42:46.497187Z","iopub.status.idle":"2024-02-18T10:42:46.501461Z","shell.execute_reply.started":"2024-02-18T10:42:46.497156Z","shell.execute_reply":"2024-02-18T10:42:46.500275Z"}}},{"cell_type":"markdown","source":"Подготовка директорий","metadata":{"id":"e4JsA1BKnDoB"}},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport shutil\nimport cv2\nimport numpy as np\nimport json\nimport matplotlib.colors\n\ndef make_directory(path):\n  if not os.path.exists(path):\n    os.mkdir(path)\n\nmake_directory(\"images\")\nmake_directory(\"annotations\")\nmake_directory(\"annotated_images\")\nmake_directory(\"masks\")","metadata":{"id":"aXl44LxAnCh-","execution":{"iopub.status.busy":"2024-05-21T16:51:47.023154Z","iopub.execute_input":"2024-05-21T16:51:47.023598Z","iopub.status.idle":"2024-05-21T16:51:47.349785Z","shell.execute_reply.started":"2024-05-21T16:51:47.023566Z","shell.execute_reply":"2024-05-21T16:51:47.347914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Вспомогательные функции","metadata":{"id":"yFgR-zDu3KBl"}},{"cell_type":"code","source":"def get_labels(path_to_xml):\n    tree = ET.parse(path_to_xml)\n    root = tree.getroot()\n    label_tags = root.findall(\".//label\")\n    labels = []\n    for label_tag in label_tags:\n      labels.append(label_tag.find(\"name\").text)\n    return labels\n\ndef get_colors(path_to_xml):\n    tree = ET.parse(path_to_xml)\n    root = tree.getroot()\n    label_tags = root.findall(\".//label\")\n    colors = []\n    for label_tag in label_tags:\n        colors.append(label_tag.find(\"color\").text)\n    return colors\n\ndef get_image_tag_attributes(image_tag):\n    image_attributes = image_tag.attrib\n    image_attributes[\"id\"] = int(image_attributes[\"id\"])\n    image_attributes[\"height\"] = int(image_attributes[\"height\"])\n    image_attributes[\"width\"] = int(image_attributes[\"width\"])\n    return image_attributes\n\ndef get_polygon_tag_attributes(polygon_tag):\n    polygon_attributes = polygon_tag.attrib\n    polygon_attributes[\"occluded\"] = int(polygon_attributes[\"occluded\"])\n    polygon_attributes[\"z_order\"] = int(polygon_attributes[\"z_order\"])\n    polygon_attributes[\"points\"] = string_points_to_array(polygon_attributes[\"points\"])\n    return polygon_attributes\n\ndef get_ellipse_tag_attributes(ellipse_tag):\n    ellipse_attributes = ellipse_tag.attrib\n    ellipse_attributes[\"cx\"] = int(float(ellipse_attributes[\"cx\"]))\n    ellipse_attributes[\"cy\"] = int(float(ellipse_attributes[\"cy\"]))\n    ellipse_attributes[\"rx\"] = int(float(ellipse_attributes[\"rx\"]))\n    ellipse_attributes[\"ry\"] = int(float(ellipse_attributes[\"ry\"]))\n    return ellipse_attributes\n\ndef get_mask_paths(path_to_masks_dir, labels):\n    mask_paths = []\n    for label in labels:\n      mask_paths.append({\"label\": label,\n                        \"path\" : os.path.join(path_to_masks_dir, f\"mask_{label}\")})\n    return mask_paths\n\ndef string_points_to_array(string_points):\n    points = []\n    for point in string_points.split(\";\"):\n      point = point.split(\",\")\n      point[0] = int(float(point[0]))\n      point[1] = int(float(point[1]))\n      points.append(point)\n    return points\n\ndef hex_to_rgb(hex_color):\n    rgb_color = matplotlib.colors.hex2color(hex_color)\n    new_rgb_color = []\n    for color in rgb_color:\n        color = int((float)(color) * 255)\n        new_rgb_color.append(color)\n    new_rgb_color[0], new_rgb_color[2] = new_rgb_color[2], new_rgb_color[0]\n    return new_rgb_color","metadata":{"id":"b1RhVNCo3S5J","execution":{"iopub.status.busy":"2024-05-21T16:51:47.351949Z","iopub.execute_input":"2024-05-21T16:51:47.353464Z","iopub.status.idle":"2024-05-21T16:51:47.374597Z","shell.execute_reply.started":"2024-05-21T16:51:47.353406Z","shell.execute_reply":"2024-05-21T16:51:47.372939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Разметка\n","metadata":{"id":"h4fDSl5ISX4N"}},{"cell_type":"code","source":"def draw_annotation(path_to_xml, path_to_images_dir, path_to_annotated_images_dir):\n    if not (os.path.exists(path_to_xml) and\n            os.path.exists(path_to_images_dir) and\n            os.path.exists(path_to_annotated_images_dir)):\n        print(\"Error! There is no such file/directory\")\n        return\n\n    images = os.listdir(path_to_images_dir)\n    for image in images:\n        shutil.copy(os.path.join(path_to_images_dir, image),\n                    os.path.join(path_to_annotated_images_dir, image))\n\n    tree = ET.parse(path_to_xml)\n    root = tree.getroot()\n    image_tags = root.findall(\"image\")\n    labels = get_labels(path_to_xml)\n    colors = get_colors(path_to_xml)\n    thickness = 1\n\n    for image_tag in image_tags:\n        image_name = image_tag.attrib.get(\"name\")\n        image_path = os.path.join(path_to_annotated_images_dir, image_name)\n        image_as_array = cv2.imread(image_path)\n\n        polygon_tags = image_tag.findall(\"polygon\")\n        for polygon_tag in polygon_tags:\n            polygon_tag_attributes = get_polygon_tag_attributes(polygon_tag)\n            points = polygon_tag_attributes[\"points\"]\n            label = polygon_tag_attributes[\"label\"]\n            hex_color = colors[labels.index(label)]\n            rgb_color = hex_to_rgb(hex_color)\n            cv2.drawContours(image_as_array, [np.array(points)], 0, rgb_color, thickness)\n\n        ellipse_tags = image_tag.findall(\"ellipse\")\n        for ellipse_tag in ellipse_tags:\n            ellipse_tag_attributes = get_ellipse_tag_attributes(ellipse_tag)\n            cx = ellipse_tag_attributes[\"cx\"]\n            cy = ellipse_tag_attributes[\"cy\"]\n            rx = ellipse_tag_attributes[\"rx\"]\n            ry = ellipse_tag_attributes[\"ry\"]\n            label = ellipse_tag_attributes[\"label\"]\n            hex_color = colors[labels.index(label)]\n            rgb_color = hex_to_rgb(hex_color)\n            cv2.ellipse(image_as_array, (cx, cy), (rx, ry), 0, 0, 360, rgb_color, thickness)\n\n        if image_as_array is not None:\n            cv2.imwrite(image_path, image_as_array)\n","metadata":{"id":"y3J-hVH3TanI","execution":{"iopub.status.busy":"2024-05-21T16:51:47.377360Z","iopub.execute_input":"2024-05-21T16:51:47.379248Z","iopub.status.idle":"2024-05-21T16:51:47.395947Z","shell.execute_reply.started":"2024-05-21T16:51:47.379188Z","shell.execute_reply":"2024-05-21T16:51:47.394848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Маски","metadata":{"id":"JtGK1lCC580F"}},{"cell_type":"code","source":"def draw_masks(path_to_xml, path_to_masks_dir):\n    if not (os.path.exists(path_to_xml) and os.path.exists(path_to_masks_dir)):\n        print(\"Error! There is no such file/directory\")\n        return\n\n    tree = ET.parse(path_to_xml)\n    root = tree.getroot()\n    image_tags = root.findall(\"image\")\n\n    for image_tag in image_tags:\n        image_tag_attributes = get_image_tag_attributes(image_tag)\n        image_name = image_tag_attributes[\"name\"]\n        width = image_tag_attributes[\"width\"]\n        height = image_tag_attributes[\"height\"]\n\n        empty_mask = np.zeros((height, width, 3), dtype=np.uint8)\n        masks = {}\n        labels = get_labels(path_to_xml)\n        for label in labels:\n            masks[label] = empty_mask\n            mask_dir = os.path.join(path_to_masks_dir, f\"mask_{label}\")\n            make_directory(mask_dir)\n\n        polygon_tags = image_tag.findall(\"polygon\")\n        for polygon_tag in polygon_tags:\n            polygon_tag_attributes = get_polygon_tag_attributes(polygon_tag)\n            points = polygon_tag_attributes[\"points\"]\n            label = polygon_tag_attributes[\"label\"]\n\n            current_mask = np.zeros((height, width, 3), dtype=np.uint8)\n            cv2.fillPoly(current_mask, [np.array(points)], (255, 255, 255))\n            masks[label] = cv2.bitwise_or(masks[label], current_mask)\n\n        ellipse_tags = image_tag.findall(\"ellipse\")\n        for ellipse_tag in ellipse_tags:\n            ellipse_tag_attributes = get_ellipse_tag_attributes(ellipse_tag)\n            cx = ellipse_tag_attributes[\"cx\"]\n            cy = ellipse_tag_attributes[\"cy\"]\n            rx = ellipse_tag_attributes[\"rx\"]\n            ry = ellipse_tag_attributes[\"ry\"]\n            label = ellipse_tag_attributes[\"label\"]\n\n            current_mask = np.zeros((height, width, 3), dtype=np.uint8)\n            cv2.ellipse(current_mask, (cx, cy), (rx, ry), 0, 0, 360, (255, 255, 255), -1)\n            masks[label] = cv2.bitwise_or(masks[label], current_mask)\n\n        for label, mask in masks.items():\n            mask_dir = os.path.join(path_to_masks_dir, f\"mask_{label.lower()}\")\n            mask_path = os.path.join(mask_dir, image_name)\n            cv2.imwrite(mask_path, mask)\n","metadata":{"id":"0JDOpnLkoinx","execution":{"iopub.status.busy":"2024-05-21T16:51:47.398650Z","iopub.execute_input":"2024-05-21T16:51:47.399632Z","iopub.status.idle":"2024-05-21T16:51:47.417387Z","shell.execute_reply.started":"2024-05-21T16:51:47.399597Z","shell.execute_reply":"2024-05-21T16:51:47.416073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"json-файл","metadata":{"id":"dh0sRWbXdZnA"}},{"cell_type":"code","source":"def create_masks_json_file(path_to_xml, path_to_images_dir, path_to_masks_dir):\n    if not (os.path.exists(path_to_xml) and\n            os.path.exists(path_to_images_dir) and\n            os.path.exists(path_to_masks_dir)):\n        print(\"Error! There is no such file/directory\")\n        return\n\n    items = []\n    tree = ET.parse(path_to_xml)\n    root = tree.getroot()\n    image_tags = root.findall(\"image\")\n    defect_id = 0\n    for image_tag in image_tags:\n        defects = []\n        polygon_tags = image_tag.findall(\"polygon\")\n        for polygon_tag in polygon_tags:\n            polygon_tag_attributes = get_polygon_tag_attributes(polygon_tag)\n            defects.append({\"id\" : defect_id,\n                            \"tag\" : \"polygon\",\n                            \"label\" : polygon_tag_attributes[\"label\"],\n                            \"points\" : polygon_tag_attributes[\"points\"]})\n            defect_id += 1\n\n        ellipse_tags = image_tag.findall(\"ellipse\")\n        for ellipse_tag in ellipse_tags:\n            ellipse_tag_attributes = get_ellipse_tag_attributes(ellipse_tag)\n            defects.append({\"id\" : defect_id,\n                            \"tag\" : \"polygon\",\n                            \"label\" : ellipse_tag_attributes[\"label\"],\n                            \"cx\" : ellipse_tag_attributes[\"cx\"],\n                            \"cy\" : ellipse_tag_attributes[\"cy\"],\n                            \"rx\" : ellipse_tag_attributes[\"rx\"],\n                            \"ry\" : ellipse_tag_attributes[\"ry\"]})\n            defect_id += 1\n\n        image_tag_attributes = get_image_tag_attributes(image_tag)\n        items.append( {\"id\" : image_tag_attributes[\"id\"],\n                      \"image\" : image_tag_attributes[\"name\"],\n                      \"width\" : image_tag_attributes[\"width\"],\n                      \"height\" : image_tag_attributes[\"height\"],\n                      \"defects\" : defects})\n\n    labels = get_labels(path_to_xml)\n    mask_paths = get_mask_paths(path_to_masks_dir, labels)\n\n    mask_info = {\n        \"datasetPath\" : path_to_images_dir,\n        \"maskPaths\" : mask_paths,\n        \"itemsCount\" : len(items),\n        \"items\" : items\n        }\n\n    path_to_json = os.path.join(path_to_images_dir, \"mask_info.json\")\n    with open(path_to_json, \"w\") as file:\n        json.dump(mask_info, file, indent = 2)\n","metadata":{"id":"wrQBLzySdaUw","execution":{"iopub.status.busy":"2024-05-21T16:51:47.419800Z","iopub.execute_input":"2024-05-21T16:51:47.420260Z","iopub.status.idle":"2024-05-21T16:51:47.436569Z","shell.execute_reply.started":"2024-05-21T16:51:47.420228Z","shell.execute_reply":"2024-05-21T16:51:47.434759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#draw_annotation(\"annotations/annotation1.xml\", \"images\", \"annotated_images\")\n#draw_annotation(\"annotations/annotation2.xml\", \"images\", \"annotated_images\")\ndraw_masks(\"/kaggle/input/timbercv-dataset/annotation1.xml\", \"masks\")\ndraw_masks(\"/kaggle/input/timbercv-dataset/annotation2.xml\", \"masks\")\ndraw_masks(\"/kaggle/input/timbercv-dataset/annotation3.xml\", \"masks\")\n#create_masks_json_file(\"annotations/annotation1.xml\", \"images\", \"masks\")\n#create_masks_json_file(\"annotations/annotation1.xml\", \"images\", \"masks\")","metadata":{"id":"phbF0jpPbGWq","outputId":"7352a670-956f-4d0c-efc8-f1c4b5f5385d","execution":{"iopub.status.busy":"2024-05-21T16:51:47.438887Z","iopub.execute_input":"2024-05-21T16:51:47.439393Z","iopub.status.idle":"2024-05-21T16:53:29.450829Z","shell.execute_reply.started":"2024-05-21T16:51:47.439353Z","shell.execute_reply":"2024-05-21T16:53:29.449389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Нейросеть","metadata":{"id":"_ruuqNa8WjJY"}},{"cell_type":"code","source":"!pip install lightning","metadata":{"id":"pzHV6EihIspU","outputId":"67a2245c-5303-4e94-f34d-0fc8061f127e","execution":{"iopub.status.busy":"2024-05-21T16:53:29.452600Z","iopub.execute_input":"2024-05-21T16:53:29.452945Z","iopub.status.idle":"2024-05-21T16:53:51.235272Z","shell.execute_reply.started":"2024-05-21T16:53:29.452918Z","shell.execute_reply":"2024-05-21T16:53:51.233828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Модель","metadata":{"id":"eIXxrEM1oKIu"}},{"cell_type":"code","source":"import lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nimport albumentations as A\nimport gc\nimport matplotlib.pyplot as plt\nimport torchmetrics\nimport IPython\nimport torchvision.models as models\n\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\n\nclass DownLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.down = nn.Sequential(\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def forward(self, x):\n        x = self.down(x)\n        return x\n\nclass UpLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode = \"bilinear\")\n        )\n        self.conv = nn.Sequential(\n            nn.Dropout(p=0.25),\n            nn.Conv2d(in_channels + out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        x = torch.cat((x, skip), dim=1)\n        x = self.conv(x)\n        return x\n\nclass DoubleConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n        )\n    def forward(self, x):\n        x = self.double_conv(x)\n        return x\n\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight = 0.005):\n        super(DiceBCELoss, self).__init__()\n        self.weight = weight\n\n    def focal_loss(self, predictions, targets):\n        gamma = 2\n        alpha = 0.005\n        p_t = targets * predictions + (1 - targets) * (1 - predictions) + 1.0e-7\n        contour_mask_index = torch.argmax(targets.sum(dim=(2, 3)), dim=1)\n        t = targets.clone()\n        t[:, contour_mask_index, :, :] *= 0\n        weights = abs(t-alpha)\n        focal_loss = -weights * ((1 - p_t)**gamma) *torch.log(p_t)   #((1 - p_t)**gamma) *\n        return focal_loss.mean()\n    \n    def forward(self, predictions, targets):   \n        contour_mask_index = torch.argmax(targets.sum(dim=(2, 3)), dim=1)\n        t = targets.clone()\n        t[:, contour_mask_index, :, :] *= 0\n        weights = abs(t - self.weight)\n        \n        p = predictions.clone()\n        p[:, contour_mask_index, :, :] *= 0\n                                   \n        dice_loss = 1 - ((2.*(weights * p * t).sum())/\n        ((weights*p).sum() + (weights*t).sum() + 1e-7))\n        \n        \n        BCE = F.binary_cross_entropy(predictions, \n                                     targets, \n                                     weight=weights, \n                                     reduction='mean')\n        \n        #BCE = self.focal_loss(predictions, targets)\n        \n        k = 0.1\n        Dice_BCE = (1 - k)*BCE + k*dice_loss\n        return Dice_BCE\n    \nclass Unet(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.example_input_array = torch.Tensor(1, 1, 640, 1936)\n        self.accuracy = torchmetrics.Accuracy(task=\"BINARY\")\n        self.train_output = None\n        self.val_output = None\n        self.loss = DiceBCELoss()\n\n        self.double_conv1 = DoubleConvLayer(1, 64)\n        self.down1 = DownLayer(64, 128)\n        self.down2 = DownLayer(128, 256)\n        self.down3 = DownLayer(256, 512)\n        self.down4 = DownLayer(512, 1024)\n        self.up1 = UpLayer(1024, 512)\n        self.up2 = UpLayer(512, 256)\n        self.up3 = UpLayer(256, 128)\n        self.up4 = UpLayer(128, 64)\n        self.double_conv2 = DoubleConvLayer(64, 13)\n\n    def forward(self, x):\n        x1 = self.double_conv1(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x5 = self.up1(x5, x4)\n        x5 = self.up2(x5, x3)\n        x5 = self.up3(x5, x2)\n        x5 = self.up4(x5, x1)\n        x5 = self.double_conv2(x5)\n        x5 = torch.sigmoid(x5)\n        return x5\n\n    \n    def dice_loss(self, predictions, targets, epsilon=1e-7):\n        loss = 1. - ((torch.sum(2 * predictions * targets) + epsilon) / \n                     (torch.sum(predictions) + torch.sum(targets) + epsilon))\n        return loss.mean()\n    \n    def focal_loss(self, predictions, targets):\n        gamma = 2\n        alpha = 0.005\n        p_t = targets * predictions + (1 - targets) * (1 - predictions) + 1.0e-7\n        contour_mask_index = torch.argmax(targets.sum(dim=(2, 3)), dim=1)\n        t = targets.clone()\n        t[:, contour_mask_index, :, :] *= 0\n        weights = abs(t-alpha)\n        focal_loss = -weights * ((1 - p_t)**gamma) *torch.log(p_t)   #((1 - p_t)**gamma) *\n        return focal_loss.mean()\n    \n    def custom_accuracy(self, x, y):\n        y_ones_amount = y.sum().item()\n        x_right_ones_amount = (x * y).sum().item()\n        ones_accuracy = x_right_ones_amount / (y_ones_amount + 1.0e-7)\n\n        contour_mask_index = torch.argmax(y.sum(dim=(2, 3)), dim=1)\n        y_contour_ones_amount = y[:,contour_mask_index,:,:].sum().item()\n        x_contour_right_ones_amount = (x[:,contour_mask_index,:,:] * y[:,contour_mask_index,:,:]).sum().item()\n        ones_accuracy_without_contour = ((x_right_ones_amount - x_contour_right_ones_amount) /\n                                        (y_ones_amount - y_contour_ones_amount + 1.0e-7))\n        return ones_accuracy, ones_accuracy_without_contour\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = self.forward(x)\n        #loss = F.binary_cross_entropy(x, y) + self.dice_loss(x, y)\n        loss = self.loss(x, y)\n        \n        result = torch.round(x)\n        accuracy = self.accuracy(result, y)\n        custom_accuracy = self.custom_accuracy(x, y)\n\n        #plt.imshow(y[0, 10,:,:].cpu())\n        #plt.show()\n\n        if self.train_output == None:\n            self.train_output = display(\"TRAIN.\", display_id=True)\n        if self.val_output == None:\n            self.val_output = display(\"VAL.\", display_id=True)\n        self.train_output.update(f\"TRAIN. acc: {accuracy:.4f}, custom_acc: {custom_accuracy[0]:.4f}, {custom_accuracy[1]:.4f}, loss: {loss:.4f}\")\n\n\n        self.log(\"loss\", loss, prog_bar=True)\n        self.log(\"accuracy\", accuracy, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x = self.forward(x)\n        #loss = F.binary_cross_entropy(x, y) + self.dice_loss(x, y)\n        loss = self.loss(x, y)\n\n        result = torch.round(x)\n        accuracy = self.accuracy(result, y)\n        custom_accuracy = self.custom_accuracy(x, y)\n\n        print(f\"TEST. acc:{accuracy:.4f}, custom_acc: {custom_accuracy[0]:.4f}, {custom_accuracy[1]:.4f}, loss: {loss:.4f}\")\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = self.forward(x)\n        #loss = F.binary_cross_entropy(x, y) + self.dice_loss(x, y)\n        loss = self.loss(x, y)\n\n        result = torch.round(x)\n        accuracy = self.accuracy(result, y)\n        custom_accuracy = self.custom_accuracy(x, y)\n\n        if self.val_output != None:\n            self.val_output.update(f\"VAL. acc: {accuracy:.4f}, custom_acc: {custom_accuracy[0]:.4f}, {custom_accuracy[1]:.4f}, loss: {loss:.4f}\")\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), \n                                     lr=3e-4)\n        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n        return optimizer #[optimizer], [scheduler]\n\n\nclass TimberDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_path, masks_path):\n        super().__init__()\n        self.transform = A.Compose([\n            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=1),\n        ])\n        self.sorted_mask_dirs = sorted(os.listdir(masks_path))\n    \n    def __len__(self):\n        image_names = [file for file in os.listdir(dataset_path) if not file.endswith(\".json\")]\n        return len(image_names)\n\n    def __getitem__(self, idx):\n        if not os.path.exists(dataset_path):\n            print(f\"Error! There is no {dataset_path} directory\")\n            return None\n        elif not os.path.exists(masks_path):\n            print(f\"Error! There is no {masks_path} directory\")\n            return None\n        else:\n            image_names = [file for file in os.listdir(dataset_path) if not file.endswith(\".json\")]\n            image_masks = []\n            for mask_dir in self.sorted_mask_dirs:\n                mask_path = os.path.join(masks_path, mask_dir, image_names[idx])\n                if os.path.exists(mask_path):\n                    mask = cv2.imread(mask_path)\n                    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n                    mask = np.round(mask/255.0)\n                    mask = np.reshape(mask, (mask.shape[0], mask.shape[1], 1))\n                    image_masks.append(mask)\n            image_path = os.path.join(dataset_path, image_names[idx])\n            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n            #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            # аугментация\n            image_masks = np.concatenate(image_masks, axis=2)\n            transformed = self.transform(image=image, mask=image_masks)\n            image = transformed[\"image\"]\n            image_masks = transformed[\"mask\"]\n\n            image = torch.tensor(image, dtype = torch.float32)\n            image_masks = torch.tensor(image_masks, dtype = torch.float32)\n            \n            image = image.unsqueeze(0)\n            #image = image.permute(2, 0, 1)\n            image_masks = image_masks.permute(2, 0, 1)\n\n            return (image, image_masks)\n\n\ndataset_path = \"/kaggle/input/timbercv-dataset/train\"\nmasks_path = \"/kaggle/working/masks\"\n\ndataset = TimberDataset(dataset_path, masks_path)\nfor directory in dataset.sorted_mask_dirs:\n    print(directory)\n\ntrain_dataset_size = int(len(dataset) * 0.85)\nval_dataset_size = int(len(dataset) * 0.10)\ntest_dataset_size = len(dataset) - train_dataset_size - val_dataset_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset,\n [train_dataset_size, val_dataset_size, test_dataset_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size = 1)\nvalid_loader = DataLoader(val_dataset, batch_size = 1)\n\n# Обучение модели\n#model = Unet()\nmodel = Unet.load_from_checkpoint(\"/kaggle/input/timbercv/pytorch/new1/2/epoch16-step4726.ckpt\")\n\nmodel.train()\ntrainer = L.Trainer(max_epochs = 200)\ntrainer.fit(model = model,\n            train_dataloaders = train_loader,\n            val_dataloaders = valid_loader)\n\n# Тестирование модели\nresult = trainer.test(model, dataloaders=DataLoader(test_dataset))\n\n# TensorBoard\n#%reload_ext tensorboard\n#%tensorboard --logdir=lightning_logs/\n","metadata":{"id":"_Z0XdkIwWlEd","outputId":"473da732-dba1-4db5-a8b7-c29ffe1f6931","_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-21T16:53:51.238226Z","iopub.execute_input":"2024-05-21T16:53:51.238671Z","iopub.status.idle":"2024-05-21T16:59:22.251832Z","shell.execute_reply.started":"2024-05-21T16:53:51.238636Z","shell.execute_reply":"2024-05-21T16:59:22.249346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"id":"AMSGhT5OMT8K","execution":{"iopub.status.busy":"2024-05-21T16:59:22.254190Z","iopub.execute_input":"2024-05-21T16:59:22.254754Z","iopub.status.idle":"2024-05-21T16:59:22.754433Z","shell.execute_reply.started":"2024-05-21T16:59:22.254709Z","shell.execute_reply":"2024-05-21T16:59:22.751848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверка","metadata":{"id":"6WVOjBFLnaI2"}},{"cell_type":"code","source":"#image_path = \"/kaggle/input/timbercv-dataset/train/2023-12-11_062607_52_2.jpg\"\n#image_path = \"/kaggle/input/timbercv-dataset/train/2023-12-20_065243_97_2.jpg\"\n#image_path = \"/kaggle/input/timbercv-dataset/test.jpg\"\n#image_path = \"/kaggle/input/timbercv-dataset/train/2023-10-27_133517_70_2.jpg\"\n\nimage_path = \"/kaggle/input/timbercv-dataset/test/2023-12-16_072036_19_2.jpg\"\n\nnp_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n#image = cv2.resize(image, (1936, 640))\n#np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2RGB)\nplt.imshow(np_image, cmap='gray')\nplt.show()\nimage = torch.tensor(np_image, dtype = torch.float32)\nplt.imshow(image.numpy(), cmap='gray')\nplt.show()\nplt.imshow(image.numpy()/255., cmap='gray')\nplt.show()\n#image = image.permute(2, 0, 1) # (640, 1936, 3) -> (3, 640, 1936)\nimage = torch.tensor(image).unsqueeze(0)\nimage = torch.tensor(image).unsqueeze(0)\n\n#model = Unet.load_from_checkpoint(\"/content/lightning_logs/version_45/checkpoints/epoch=6-step=980.ckpt\")\nmodel.eval()\nmodel.cpu()\n\nwith torch.no_grad():\n    result = model(image)","metadata":{"id":"eF1IQ-2QlmHM","execution":{"iopub.status.busy":"2024-05-21T16:59:22.761723Z","iopub.execute_input":"2024-05-21T16:59:22.762966Z","iopub.status.idle":"2024-05-21T16:59:25.831588Z","shell.execute_reply.started":"2024-05-21T16:59:22.762915Z","shell.execute_reply":"2024-05-21T16:59:25.829095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = torch.round(result)*255\n\n#del zip\nfor mask, label in zip(result[0], dataset.sorted_mask_dirs):\n    print(label)\n    plt.imshow(mask, cmap='gray')\n    plt.show()","metadata":{"id":"b1fh5gzpvDrn","execution":{"iopub.status.busy":"2024-05-21T16:59:25.832562Z","iopub.status.idle":"2024-05-21T16:59:25.833054Z","shell.execute_reply.started":"2024-05-21T16:59:25.832821Z","shell.execute_reply":"2024-05-21T16:59:25.832838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Отрисовка масок на изображении","metadata":{}},{"cell_type":"code","source":"# Определение массива цветов для контуров масок\ncolors = [(255, 0, 0), (0, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 255),\n          (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n          (128, 0, 128), (0, 128, 128), (255, 128, 0)]  \nmasks_np = result.squeeze(0).cpu().numpy()\nimage_with_contours = np_image.copy()\n\n# Отображение изображения до рисования контуров\nplt.imshow(image_with_contours, cmap='gray')\nplt.show()\n\n# Рисование контуров каждой маски с определенным цветом из массива colors\nfor idx, mask in enumerate(masks_np):\n    # Получение контуров маски\n    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Использование соответствующего цвета из массива colors\n    color = colors[idx % len(colors)]  # Использование цвета по модулю от индекса маски\n    cv2.drawContours(image_with_contours, contours, -1, color, 2)\n\n# Отображение изображения с контурами масок\nplt.imshow(image_with_contours)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.835318Z","iopub.status.idle":"2024-05-21T16:59:25.835916Z","shell.execute_reply.started":"2024-05-21T16:59:25.835645Z","shell.execute_reply":"2024-05-21T16:59:25.835665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сохранение модели","metadata":{"id":"R3WkXqFwqNze"}},{"cell_type":"code","source":"model.eval()\ntorch.save(model.state_dict(), 'model_weights.pth')\n\nmodel_scripted = torch.jit.script(model) # Export to TorchScript\nmodel_scripted.save('unet.pt') # Save","metadata":{"id":"rbAgK8tVqI-d","execution":{"iopub.status.busy":"2024-05-21T16:59:25.837754Z","iopub.status.idle":"2024-05-21T16:59:25.838289Z","shell.execute_reply.started":"2024-05-21T16:59:25.838049Z","shell.execute_reply":"2024-05-21T16:59:25.838068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\nimport zipfile\n# директория, которую необходимо скачать\ndirectory_to_download = '/kaggle/working/lightning_logs/version_0'\n\nzip_file_path = 'files_to_download.zip'\nwith zipfile.ZipFile(zip_file_path, 'w') as zipf:\n    for root, dirs, files in os.walk(directory_to_download):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory_to_download, '..')))\n\nfrom IPython.display import FileLink\nFileLink(zip_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.840146Z","iopub.status.idle":"2024-05-21T16:59:25.840640Z","shell.execute_reply.started":"2024-05-21T16:59:25.840405Z","shell.execute_reply":"2024-05-21T16:59:25.840442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PyTorch to TFLite","metadata":{}},{"cell_type":"code","source":"!pip install onnx_tf","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.842080Z","iopub.status.idle":"2024-05-21T16:59:25.842536Z","shell.execute_reply.started":"2024-05-21T16:59:25.842321Z","shell.execute_reply":"2024-05-21T16:59:25.842338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nclass Unet(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.example_input_array = torch.Tensor(1, 3, 640, 1936)\n        self.accuracy = torchmetrics.Accuracy(task=\"BINARY\")\n        self.train_output = None\n        self.val_output = None\n        self.loss = DiceBCELoss()\n\n        self.double_conv1 = DoubleConvLayer(3, 64)\n        self.down1 = DownLayer(64, 128)\n        self.down2 = DownLayer(128, 256)\n        self.up1 = UpLayer(256, 128)\n        self.up2 = UpLayer(128, 64)\n        self.double_conv2 = DoubleConvLayer(64, 13)\n\n    def forward(self, x):\n        x1 = self.double_conv1(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x3 = self.up1(x3, x2)\n        x3 = self.up2(x3, x1)\n        x3 = self.double_conv2(x3)\n        x3 = torch.sigmoid(x3)\n        return x3\n\n    \n    def dice_loss(self, predictions, targets, epsilon=1e-7):\n        loss = 1. - ((torch.sum(2 * predictions * targets) + epsilon) / \n                     (torch.sum(predictions) + torch.sum(targets) + epsilon))\n        return loss.mean()\n    \n    def focal_loss(self, predictions, targets):\n        gamma = 2\n        alpha = 0.005\n        p_t = targets * predictions + (1 - targets) * (1 - predictions) + 1.0e-7\n        contour_mask_index = torch.argmax(targets.sum(dim=(2, 3)), dim=1)\n        t = targets.clone()\n        t[:, contour_mask_index, :, :] *= 0\n        weights = abs(t-alpha)\n        focal_loss = -weights * ((1 - p_t)**gamma) *torch.log(p_t)   #((1 - p_t)**gamma) *\n        return focal_loss.mean()\n    \n    def custom_accuracy(self, x, y):\n        y_ones_amount = y.sum().item()\n        x_right_ones_amount = (x * y).sum().item()\n        ones_accuracy = x_right_ones_amount / (y_ones_amount + 1.0e-7)\n\n        contour_mask_index = torch.argmax(y.sum(dim=(2, 3)), dim=1)\n        y_contour_ones_amount = y[:,contour_mask_index,:,:].sum().item()\n        x_contour_right_ones_amount = (x[:,contour_mask_index,:,:] * y[:,contour_mask_index,:,:]).sum().item()\n        ones_accuracy_without_contour = ((x_right_ones_amount - x_contour_right_ones_amount) /\n                                        (y_ones_amount - y_contour_ones_amount + 1.0e-7))\n        return ones_accuracy, ones_accuracy_without_contour\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = self.forward(x)\n        #loss = F.binary_cross_entropy(x, y) + self.dice_loss(x, y)\n        loss = self.loss(x, y)\n        \n        result = torch.round(x)\n        accuracy = self.accuracy(result, y)\n        custom_accuracy = self.custom_accuracy(x, y)\n\n        #plt.imshow(y[0, 10,:,:].cpu())\n        #plt.show()\n\n        if self.train_output == None:\n            self.train_output = display(\"TRAIN.\", display_id=True)\n        if self.val_output == None:\n            self.val_output = display(\"VAL.\", display_id=True)\n        self.train_output.update(f\"TRAIN. acc: {accuracy:.4f}, custom_acc: {custom_accuracy[0]:.4f}, {custom_accuracy[1]:.4f}, loss: {loss:.4f}\")\n\n\n        self.log(\"loss\", loss, prog_bar=True)\n        self.log(\"accuracy\", accuracy, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x = self.forward(x)\n        #loss = F.binary_cross_entropy(x, y) + self.dice_loss(x, y)\n        loss = self.loss(x, y)\n\n        result = torch.round(x)\n        accuracy = self.accuracy(result, y)\n        custom_accuracy = self.custom_accuracy(x, y)\n\n        print(f\"TEST. acc:{accuracy:.4f}, custom_acc: {custom_accuracy[0]:.4f}, {custom_accuracy[1]:.4f}, loss: {loss:.4f}\")\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = self.forward(x)\n        #loss = F.binary_cross_entropy(x, y) + self.dice_loss(x, y)\n        loss = self.loss(x, y)\n\n        result = torch.round(x)\n        accuracy = self.accuracy(result, y)\n        custom_accuracy = self.custom_accuracy(x, y)\n\n        if self.val_output != None:\n            self.val_output.update(f\"VAL. acc: {accuracy:.4f}, custom_acc: {custom_accuracy[0]:.4f}, {custom_accuracy[1]:.4f}, loss: {loss:.4f}\")\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), \n                                     lr=3e-4)\n        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n        return optimizer #[optimizer], [scheduler]\n'''","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.844528Z","iopub.status.idle":"2024-05-21T16:59:25.845003Z","shell.execute_reply.started":"2024-05-21T16:59:25.844789Z","shell.execute_reply":"2024-05-21T16:59:25.844806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport onnx\nimport tensorflow as tf\nimport onnx_tf\n\n# Load the PyTorch model\n#pytorch_model = model\n#pytorch_model = Unet.load_from_checkpoint(\"/kaggle/input/timbercv_mobile/pytorch/0/1/epoch0-step288.ckpt\")\npytorch_model = Unet.load_from_checkpoint(\"/kaggle/input/timbercv/pytorch/8/1/epoch54-step15840.ckpt\")\npytorch_model.eval()\n\n# Export the PyTorch model to ONNX format\ninput_shape = (1, 3, 640, 1936)\nsample_input = torch.randn(input_shape)\nonnx_model_path = 'unet.onnx'\ntorch.onnx.export(\n    model,                  # PyTorch Model\n    sample_input,                    # Input tensor\n    onnx_model_path,        # Output file (eg. 'output_model.onnx')\n    opset_version=12,       # Operator support version\n    input_names=['input'],   # Input tensor name (arbitary)\n    output_names=['output'] # Output tensor name (arbitary)\n)\n# Load the ONNX model\nonnx_model = onnx.load(onnx_model_path)\n\n# Check that the IR is well formed\nonnx.checker.check_model(onnx_model)\n\n# Print a Human readable representation of the graph\nonnx.helper.printable_graph(onnx_model.graph)\n\n# Convert the ONNX model to TensorFlow format\ntf_model_path = 'unet.pb'\ntf_rep = onnx_tf.backend.prepare(onnx_model)\ntf_rep.export_graph(tf_model_path)\n\ntf_model = tf.keras.models.load_model(tf_model_path)\n\n# Convert the TensorFlow model to TensorFlow Lite format\nconverter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(tf_model_path)\n#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n#converter.target_spec.supported_ops = [tf.float32]\ntflite_model = converter.convert()\n\n# Save the TensorFlow Lite model to a file\nwith open('unet.tflite', 'wb') as f:\n    f.write(tflite_model)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.846746Z","iopub.status.idle":"2024-05-21T16:59:25.847247Z","shell.execute_reply.started":"2024-05-21T16:59:25.847010Z","shell.execute_reply":"2024-05-21T16:59:25.847029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_to_zip = '/kaggle/working/unet.tflite'\nzip_file_path = 'TFLite_model.zip'\n\nwith zipfile.ZipFile(zip_file_path, 'w') as zipf:\n    zipf.write(file_to_zip, os.path.basename(file_to_zip))\n\nFileLink(zip_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.848522Z","iopub.status.idle":"2024-05-21T16:59:25.848979Z","shell.execute_reply.started":"2024-05-21T16:59:25.848762Z","shell.execute_reply":"2024-05-21T16:59:25.848779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Тест модели TFLite","metadata":{}},{"cell_type":"code","source":"# Загрузка модели TensorFlow Lite\ninterpreter = tf.lite.Interpreter(model_path=\"/kaggle/working/unet.tflite\")\ninterpreter.allocate_tensors()\n\n# Получение информации о входных и выходных тензорах\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Подготовка входных данных \nimage_path = \"/kaggle/input/timbercv-dataset/train/2023-12-11_062607_52_2.jpg\"\n#image_path = \"/kaggle/input/timbercv-dataset/train/2023-12-20_065243_97_2.jpg\"\n#image_path = \"/kaggle/input/timbercv-dataset/test.jpg\"\n\nimage = cv2.imread(image_path)\n#image = cv2.resize(image, (1936, 640))\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image)\nplt.show()\nimage = torch.tensor(image, dtype = torch.float32)\nimage = image.permute(2, 0, 1) # (640, 1936, 3) -> (3, 640, 1936)\nimage = torch.tensor(image).unsqueeze(0)\n\n# Загрузка входных данных в интерпретатор\ninterpreter.set_tensor(input_details[0]['index'], image)\n\n# Выполнение модели\ninterpreter.invoke()\n\n# Получение результатов\noutput_data = interpreter.get_tensor(output_details[0]['index'])\noutput_data = np.round(output_data)*255\n\nfor mask, label in zip(output_data[0], dataset.sorted_mask_dirs):\n    print(label)\n    plt.imshow(mask)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.850619Z","iopub.status.idle":"2024-05-21T16:59:25.851128Z","shell.execute_reply.started":"2024-05-21T16:59:25.850890Z","shell.execute_reply":"2024-05-21T16:59:25.850908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Отрисовка масок","metadata":{}},{"cell_type":"code","source":"# Определение массива цветов для контуров масок\ncolors = [(255, 0, 0), (0, 255, 0), (0, 255, 0), (0, 255, 255), (255, 0, 255),\n          (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),\n          (128, 0, 128), (0, 128, 128), (255, 128, 0)]  \nmasks_np = result.squeeze(0).cpu().numpy()\nimage_with_contours = np_image.copy()\n\n# Отображение изображения до рисования контуров\nplt.imshow(image_with_contours)\nplt.show()\n\n# Рисование контуров каждой маски с определенным цветом из массива colors\nfor idx, mask in enumerate(masks_np):\n    # Получение контуров маски\n    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Использование соответствующего цвета из массива colors\n    color = colors[idx % len(colors)]  # Использование цвета по модулю от индекса маски\n    cv2.drawContours(image_with_contours, contours, -1, color, 2)\n\n# Отображение изображения с контурами масок\nplt.imshow(image_with_contours)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T16:59:25.852980Z","iopub.status.idle":"2024-05-21T16:59:25.853470Z","shell.execute_reply.started":"2024-05-21T16:59:25.853250Z","shell.execute_reply":"2024-05-21T16:59:25.853266Z"},"trusted":true},"execution_count":null,"outputs":[]}]}